{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed9e938f",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e914a",
   "metadata": {},
   "source": [
    " ### Q4. Take any text file and now your task is to Text Summarization without using hugging transformer library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97c6a4",
   "metadata": {},
   "source": [
    "Ans 4. To perform text summarization without using the Hugging Face Transformers library, you can use a simple extractive approach. Here's an example implementation using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956f32e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Whether extractive or abstractive, text summarization can be a valuable tool for handling large volumes of information and improving the efficiency of information processing tasks. Text summarization has many applications, including aiding in information retrieval, helping users quickly grasp the main points of a document, and enabling efficient storage and analysis of large volumes of text data. They involve tasks such as text preprocessing, sentence or phrase scoring, sentence selection, and text generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Tokenize the sentences into words\n",
    "    words = [word.lower() for sentence in sentences for word in word_tokenize(sentence)]\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def generate_summary(text, num_sentences=3):\n",
    "    # Preprocess the text\n",
    "    words = preprocess_text(text)\n",
    "    \n",
    "    # Calculate word frequencies\n",
    "    word_freq = FreqDist(words)\n",
    "    \n",
    "    # Assign scores to sentences based on word frequencies\n",
    "    sentences = sent_tokenize(text)\n",
    "    scores = []\n",
    "    for sentence in sentences:\n",
    "        score = sum(word_freq[word.lower()] for word in word_tokenize(sentence) if word in word_freq)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Sort sentences based on scores\n",
    "    ranked_sentences = sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
    "    \n",
    "    # Select the top sentences for the summary\n",
    "    summary_sentences = [sentence for _, sentence in ranked_sentences[:num_sentences]]\n",
    "    \n",
    "    # Join the summary sentences into a single string\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Read the text from a file\n",
    "with open('example.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Generate the summary\n",
    "summary = generate_summary(text)\n",
    "\n",
    "# Print the summary\n",
    "print(\"Summary:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8cb40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
