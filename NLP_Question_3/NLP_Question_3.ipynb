{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ad7c04",
   "metadata": {},
   "source": [
    "# NLP_Question_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d792b",
   "metadata": {},
   "source": [
    "### Q3. From question 2, As you got the CSV and now you need perform key word extraction from that csv file and do the Topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3ec9e",
   "metadata": {},
   "source": [
    "Ans 3.To perform keyword extraction and topic modeling on the CSV file containing the extracted text, you can use various natural language processing (NLP) libraries in Python such as NLTK, spaCy, or Gensim. Here's an example using the NLTK library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88e2bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\acer\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\acer\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\acer\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\acer\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "502c8486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: write, on, file, usp, gpa\n",
      "Topic 2: write, on, file, usp, gpa\n",
      "Topic 3: note, link, https, com, datasetthisisthedatasetyoucanusethisdatasetforthisquestion\n",
      "Topic 4: write, on, file, usp, gpa\n",
      "Topic 5: write, on, file, usp, gpa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('pdf_text_with_word_counts.csv')\n",
    "\n",
    "# Extract the words and their frequencies\n",
    "words = df['Word']\n",
    "\n",
    "# Create a corpus with repeated words based on their frequencies\n",
    "corpus = []\n",
    "for word in words:\n",
    "    corpus.append(str(word))  # Convert to string\n",
    "\n",
    "# Convert the corpus into a list of documents\n",
    "documents = [' '.join(corpus)]\n",
    "\n",
    "# Perform keyword extraction using TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Extract the feature names (keywords)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Perform topic modeling using Latent Dirichlet Allocation (LDA)\n",
    "num_topics = 5  # Specify the number of topics\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Get the top keywords for each topic\n",
    "top_keywords = []\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_keywords.append([feature_names[i] for i in topic.argsort()[:-6:-1]])\n",
    "\n",
    "# Print the top keywords for each topic\n",
    "for topic_idx, keywords in enumerate(top_keywords):\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(keywords)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed561d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
